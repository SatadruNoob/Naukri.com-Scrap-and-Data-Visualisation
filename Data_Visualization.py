# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OdqW8MWVoVoFM2Qzdmq7wK8ZHEyOh2zX
"""

# Import necessary library
import pandas as pd

# Description:
# This script performs one-hot encoding on the 'Location' column of the final cleaned Excel file
# generated after the completion of all three procedures in the previous script.
# The final output from the previous script is named 'Final_Cleaned_File.xlsx'.
# This one-hot encoding script will process the combined job listings data to transform
# the 'Location' column into separate rows for each location.

# Load the final cleaned Excel file
file_path = '/content/Final_Cleaned_File.xlsx'
df = pd.read_excel(file_path)

# Print the original dimensions of the DataFrame
original_shape = df.shape
print(f"Original DataFrame: {original_shape[0]} rows x {original_shape[1]} columns")

# Function to perform one-hot encoding for the 'Location' column
def one_hot_encode_location(row):
    locations = [loc.strip() for loc in row['Location'].split(',')]
    rows = [row.copy() for _ in locations]
    for i, loc in enumerate(locations):
        rows[i]['Location'] = loc
    return rows

# Apply the one-hot encoding function to each row and flatten the list of lists
new_rows = []
for index, row in df.iterrows():
    new_rows.extend(one_hot_encode_location(row))

# Create a new DataFrame from the new rows
new_df = pd.DataFrame(new_rows)

# Save the new DataFrame with one-hot encoded 'Location' to a new Excel file
new_file_path = '/content/combined_job_listings_one_hot_encoded.xlsx'
new_df.to_excel(new_file_path, index=False)

# Print the new dimensions of the DataFrame
new_shape = new_df.shape
print(f"New DataFrame: {new_shape[0]} rows x {new_shape[1]} columns")

# Calculate and print the increase in data volume
increase_in_rows = new_shape[0] - original_shape[0]
increase_in_columns = new_shape[1] - original_shape[1]
print(f"Increase in rows: {increase_in_rows}")
print(f"Increase in columns: {increase_in_columns}")
print(f"Data volume increased by: {new_shape[0] / original_shape[0]:.2f} times")

# Note:
# - This script should be executed after the completion of the web scraping, Excel file compilation,
#   and data cleaning processes described in the previous script.
# - The output file from this script, 'Final_Cleaned_File_one_hot_encoded.xlsx', will contain the
#   one-hot encoded data, where each location from the original 'Location' column is represented
#   in a separate row, suitable for further analysis.

#Top Locations Contributing to Data Skewness

import pandas as pd
import matplotlib.pyplot as plt

# Load the Excel file with one-hot encoded data
file_path = '/content/combined_job_listings_one_hot_encoded.xlsx'
df = pd.read_excel(file_path)

# Count occurrences of each location
location_counts = df['Location'].value_counts()

# Determine the number of top locations to display
top_n = 10  # You can adjust this number as needed

# Select the top N locations
top_locations = location_counts.head(top_n)

# Plot histogram for top locations
plt.figure(figsize=(10, 6))
top_locations.plot(kind='bar')
plt.title(f'Top {top_n} Locations Contributing to Data Skewness')
plt.xlabel('Location')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

#Top Locations Contributing to Data Skewness with 'Years of Experience' up to 8



import pandas as pd
import matplotlib.pyplot as plt

# Load the Excel file with one-hot encoded data
file_path = '/content/combined_job_listings_one_hot_encoded.xlsx'
df = pd.read_excel(file_path)

# Filter the DataFrame to include only rows with 'Years of Experience' up to 8
filtered_df = df[df['Years of Experience'] <= 8]

# Count occurrences of each value in the 'Location' column
location_counts = filtered_df['Location'].value_counts()

# Determine the number of top locations to display
top_n = 12  # You can adjust this number as needed

# Select the top N locations
top_locations = location_counts.head(top_n)

# Plot histogram for top locations
plt.figure(figsize=(10, 6))
top_locations.plot(kind='bar')
plt.title(f'Top {top_n} Locations Contributing to Data Skewness (Years of Experience <= 8)')
plt.xlabel('Location')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

#Word clouds for each top location


import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
import seaborn as sns

# Load the Excel file with one-hot encoded data
file_path = '/content/combined_job_listings_one_hot_encoded.xlsx'
df = pd.read_excel(file_path)

# Filter the DataFrame to include only rows with 'Years of Experience' up to 8
filtered_df = df[df['Years of Experience'] <= 17]

# Count occurrences of each value in the 'Location' column
location_counts = filtered_df['Location'].value_counts()

# Determine the number of top locations to display
top_n_locations = 5  # You can adjust this number as needed

# Select the top N locations
top_locations = location_counts.head(top_n_locations).index

# Function to clean and tokenize skill sets
def tokenize_skills(skill_set):
    tokens = skill_set.lower().split(',')
    return [token.strip() for token in tokens if token.strip() not in STOPWORDS]

# Create a dictionary to store skill frequencies for each top location
location_skill_map = {location: Counter() for location in top_locations}

# Tokenize skills and count frequencies for each top location
for location in top_locations:
    location_data = filtered_df[filtered_df['Location'] == location]
    for skills in location_data['Skill Set']:
        tokens = tokenize_skills(skills)
        location_skill_map[location].update(tokens)

# Determine the number of top skills to display for each location
top_m_skills = 5  # You can adjust this number as needed

# Create word clouds for each top location
plt.figure(figsize=(10, 5))
for i, (location, skill_counter) in enumerate(location_skill_map.items()):
    plt.subplot(2, 3, i + 1)  # Adjust the subplot grid as needed
    top_skills = dict(skill_counter.most_common(top_m_skills))
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_skills)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.title(f'Top Skills for {location}')
    plt.axis('off')

plt.tight_layout()
plt.show()

#Top Skills by Location (Years of Experience <= 17)


import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import STOPWORDS

# Load the Excel file with one-hot encoded data
file_path = '/content/combined_job_listings_one_hot_encoded.xlsx'
df = pd.read_excel(file_path)

# Filter the DataFrame to include only rows with 'Years of Experience' up to 17
filtered_df = df[df['Years of Experience'] <= 17]

# Count occurrences of each value in the 'Location' column
location_counts = filtered_df['Location'].value_counts()

# Determine the number of top locations to display
top_n_locations = 6  # You can adjust this number as needed

# Select the top N locations
top_locations = location_counts.head(top_n_locations).index

# Function to clean and tokenize skill sets
def tokenize_skills(skill_set):
    if pd.isna(skill_set):
        return []
    tokens = skill_set.lower().split(',')
    return [token.strip() for token in tokens if token.strip() not in STOPWORDS]

# Create a dictionary to store skill frequencies for each top location
location_skill_map = {location: Counter() for location in top_locations}

# Tokenize skills and count frequencies for each top location
for location in top_locations:
    location_data = filtered_df[filtered_df['Location'] == location]
    for skills in location_data['Skill Set']:
        tokens = tokenize_skills(skills)
        location_skill_map[location].update(tokens)

# Determine the number of top skills to display for each location
top_m_skills = 14  # You can adjust this number as needed

# Create a DataFrame for the heatmap
heatmap_data = pd.DataFrame(index=top_locations)

for location, skill_counter in location_skill_map.items():
    top_skills = dict(skill_counter.most_common(top_m_skills))
    heatmap_data = heatmap_data.join(pd.DataFrame(top_skills, index=[location]).T, how='outer')

# Fill NaN values with 0
heatmap_data = heatmap_data.fillna(0)

# Plot heatmap
plt.figure(figsize=(14, 8))
sns.heatmap(heatmap_data, annot=True, fmt=".2f", cmap="YlGnBu")
plt.title('Top Skills by Location (Years of Experience <= 17)')
plt.xlabel('Skill')
plt.ylabel('Location')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

#Top Roles by Location

import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# Load the Excel file with one-hot encoded data
file_path = '/content/combined_job_listings_one_hot_encoded.xlsx'
df = pd.read_excel(file_path)

# Filter the DataFrame to include only rows with 'Years of Experience' up to 17
filtered_df = df[df['Years of Experience'] <= 17]

# Count occurrences of each value in the 'Location' column
location_counts = filtered_df['Location'].value_counts()

# Determine the number of top locations to display
top_m_locations = 6  # You can adjust this number as needed

# Select the top M locations
top_locations = location_counts.head(top_m_locations).index

# Define a list of common job roles (this is a simplified example, adjust as needed)
common_job_roles = [
    'developer', 'engineer', 'manager', 'analyst', 'consultant', 'administrator', 'architect',
    'specialist', 'coordinator', 'director', 'officer', 'technician', 'executive', 'strategist',
    'designer', 'operator', 'planner', 'supervisor', 'programmer'
]

# Function to clean and tokenize job descriptions
def extract_roles(job_description):
    if pd.isna(job_description):
        return []
    tokens = job_description.lower().split()
    roles = [token.strip(',.!?:;()[]{}') for token in tokens if token.strip() in common_job_roles]
    return roles

# Create a dictionary to store role frequencies for each top location
location_role_map = {location: Counter() for location in top_locations}

# Tokenize roles and count frequencies for each top location
for location in top_locations:
    location_data = filtered_df[filtered_df['Location'] == location]
    for description in location_data['Job Description']:
        roles = extract_roles(description)
        location_role_map[location].update(roles)

# Determine the number of top roles to display for each location
top_n_roles = 10  # You can adjust this number as needed

# Create a DataFrame for the heatmap
heatmap_role_data = pd.DataFrame(index=top_locations)

for location, role_counter in location_role_map.items():
    top_roles = dict(role_counter.most_common(top_n_roles))
    heatmap_role_data = heatmap_role_data.join(pd.DataFrame(top_roles, index=[location]).T, how='outer')

# Fill NaN values with 0
heatmap_role_data = heatmap_role_data.fillna(0)

# Plot heatmap for roles
plt.figure(figsize=(14, 8))
sns.heatmap(heatmap_role_data, annot=True, fmt=".2f", cmap="YlGnBu")
plt.title('Top Roles by Location (Years of Experience <= 17)')
plt.xlabel('Role')
plt.ylabel('Location')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

#Top Skill Set for Different Locations where Years of Experience <= 8)


import pandas as pd
from collections import Counter
import re
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# Load the Excel file with one-hot encoded data
file_path = '/content/combined_job_listings_one_hot_encoded.xlsx'
df = pd.read_excel(file_path)

# Filter the DataFrame to include only rows with 'Years of Experience' up to 8
filtered_df = df[df['Years of Experience'] <= 8]

# Select specific locations
selected_locations = ['Bengaluru', 'Hyderabad', 'Mumbai', 'Pune']

# Function to clean and tokenize skill sets for Skill Set
def clean_and_tokenize_skill_set(skill_set):
    if pd.isna(skill_set):
        return []
    # Remove punctuation, convert to lowercase, and tokenize based on word boundaries
    cleaned_text = re.findall(r'\b\w+\b', skill_set.lower())
    # Filter out stop words and short words
    tokens = [token for token in cleaned_text if token not in ENGLISH_STOP_WORDS and len(token) > 1]
    return tokens

# Function to generate bar plots for selected locations and the Skill Set column
def generate_bar_plot_for_location(df, location, column_name, tokenizer_func, top_percentage):
    location_data = df[df['Location'] == location]
    all_tokens = []
    for text in location_data[column_name]:
        tokens = tokenizer_func(text)
        all_tokens.extend(tokens)

    # Count frequencies of tokens
    token_counter = Counter(all_tokens)

    # Calculate cumulative frequency to get the top tokens
    token_counts = pd.Series(token_counter).sort_values(ascending=False)
    cumulative_freq_tokens = token_counts.cumsum() / token_counts.sum()
    top_tokens = token_counts[cumulative_freq_tokens <= top_percentage].index

    # Plot bar plot
    plt.figure(figsize=(10, 5))  # Adjust width to 10 inches and height to 5 inches
    sns.barplot(x=token_counts[top_tokens].values, y=token_counts[top_tokens].index, palette="viridis")
    plt.title(f'Top {column_name} Tokens for {location} (Top {int(top_percentage*100)}%, Years of Experience <= 8)')
    plt.xlabel('Frequency')
    plt.ylabel('Token')
    plt.tight_layout()
    plt.show()

# Generate bar plots for each selected location for Skill Set using single tokens (top 50%)
for location in selected_locations:
    generate_bar_plot_for_location(filtered_df, location, 'Skill Set', clean_and_tokenize_skill_set, 0.50)